import os
import json
import logging
import time
from tqdm import tqdm
from dotenv import load_dotenv
import pandas as pd
from datetime import datetime
import yaml

# --- å°Žå…¥æ‰€æœ‰å¿…è¦çš„æ¨¡çµ„å’Œé¡žåˆ¥ ---
from src.utils.config_loader import load_config
from src.core.paper_aligned_race_executor import PaperAlignedRACEExecutor
from src.core.reasoning_task_constructor import ReasoningTaskConstructor
from src.modules.paper_aligned_gain_guided_exploration import PaperAlignedGainGuidedExploration
from src.modules.true_information_gain_calculator import TrueInformationGainCalculator
from src.modules.true_self_play_optimizer import TrueSelfPlayOptimizer
from src.modules.rejection_feedback import RejectionFeedback
from src.modules.rejection_detector import RejectionDetector
from models.closedsource.gpt4_wrapper import GPT4Wrapper
from models.opensource.qwen.qwen_wrapper import QwenWrapper
from src.evaluation.paper_compliant_evaluation import RACESystemEvaluationAdapter

def create_timestamped_experiment_session():
    """å‰µå»ºå¸¶æ™‚é–“æˆ³è¨˜çš„å¯¦é©—æœƒè©±"""
    
    # ç”Ÿæˆæ™‚é–“æˆ³è¨˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # å‰µå»ºå¯¦é©—ç›®éŒ„çµæ§‹
    base_dir = "experiments"
    session_dir = f"{base_dir}/exp_{timestamp}"
    
    # å‰µå»ºç›®éŒ„çµæ§‹
    dirs_to_create = [
        session_dir,
        f"{session_dir}/attack_details",
        f"{session_dir}/logs", 
        f"{session_dir}/configs",
        f"{session_dir}/results"
    ]
    
    for dir_path in dirs_to_create:
        os.makedirs(dir_path, exist_ok=True)
    
    return timestamp, session_dir

def setup_experiment_logging(session_dir: str, timestamp: str):
    """è¨­ç½®å¯¦é©—å°ˆç”¨çš„æ—¥èªŒè¨˜éŒ„"""
    
    # æ¸…é™¤ä¹‹å‰çš„æ—¥èªŒé…ç½®
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    # å‰µå»ºæ—¥èªŒæ–‡ä»¶
    log_file = f"{session_dir}/logs/experiment_{timestamp}.log"
    
    # é…ç½®æ—¥èªŒè¨˜éŒ„å™¨
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()  # åŒæ™‚è¼¸å‡ºåˆ°æŽ§åˆ¶å°
        ]
    )
    
    return log_file

def save_experiment_metadata(session_dir: str, config: dict, dataset_info: dict):
    """ä¿å­˜å¯¦é©—å…ƒæ•¸æ“š"""
    
    metadata = {
        "experiment_info": {
            "timestamp": datetime.now().isoformat(),
            "session_id": os.path.basename(session_dir),
            "python_version": "3.8+",
            "race_version": "paper_aligned_v1.0"
        },
        "dataset_info": dataset_info,
        "model_config": {
            "attacker": config['models']['attacker']['name'],
            "target": config['models']['target']['name'], 
            "judge": config['models']['judge']['name']
        },
        "attack_config": {
            "max_turns": config['attack_state_machine']['max_turns'],
            "num_candidates": config['gain_guided_exploration']['num_candidates'],
            "num_simulations": config['self_play']['num_simulations']
        }
    }
    
    # ä¿å­˜å…ƒæ•¸æ“š
    metadata_file = f"{session_dir}/experiment_metadata.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=4)
    
    # å‚™ä»½é…ç½®æ–‡ä»¶
    config_backup = f"{session_dir}/configs/config_backup.yaml"
    with open(config_backup, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
    
    return metadata_file

def generate_experiment_summary(session_dir: str, all_attack_results: list, 
                               final_evaluation: dict, execution_time: float):
    """ç”Ÿæˆå¯¦é©—ç¸½çµå ±å‘Š"""
    
    # çµ±è¨ˆä¿¡æ¯
    total_attacks = len(all_attack_results)
    successful_attacks = sum(1 for r in all_attack_results 
                           if r.get('attack_outcome', {}).get('attack_successful', False))
    
    # è¨ˆç®—å¹³å‡è¼ªæ•¸
    total_turns = sum(r.get('execution_metadata', {}).get('actual_turns', 0) 
                     for r in all_attack_results)
    avg_turns = total_turns / total_attacks if total_attacks > 0 else 0
    
    # æ¨¡çµ„ä½¿ç”¨çµ±è¨ˆ
    module_stats = {
        'gge_usage': sum(r.get('module_usage_statistics', {}).get('gge_usage', 0) 
                        for r in all_attack_results),
        'self_play_usage': sum(r.get('module_usage_statistics', {}).get('self_play_usage', 0) 
                              for r in all_attack_results),
        'rejection_feedback_usage': sum(r.get('module_usage_statistics', {}).get('rejection_feedback_usage', 0) 
                                       for r in all_attack_results)
    }
    
    # ç”Ÿæˆç¸½çµ
    summary = {
        "experiment_summary": {
            "session_id": os.path.basename(session_dir),
            "completion_time": datetime.now().isoformat(),
            "total_execution_time_minutes": round(execution_time / 60, 2)
        },
        "attack_statistics": {
            "total_attacks": total_attacks,
            "successful_attacks": successful_attacks,
            "success_rate": round(successful_attacks / total_attacks * 100, 2) if total_attacks > 0 else 0,
            "average_turns_per_attack": round(avg_turns, 2)
        },
        "module_usage_statistics": module_stats,
        "llm_judge_evaluation": final_evaluation.get('evaluation_summary', {}),
        "paper_alignment": {
            "avg_compliance_score": round(
                sum(r.get('paper_alignment_analysis', {}).get('paper_compliance_score', 0) 
                    for r in all_attack_results) / total_attacks, 3
            ) if total_attacks > 0 else 0
        }
    }
    
    # ä¿å­˜ç¸½çµ
    summary_file = f"{session_dir}/EXPERIMENT_SUMMARY.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=4)
    
    return summary_file

def create_model(model_key: str, config: dict):
    """
    æ ¹æ“šè¨­å®šæª”ä¸­çš„éµåï¼Œå‰µå»ºå°æ‡‰çš„æ¨¡åž‹å¯¦ä¾‹ã€‚
    """
    # å¾ž config ä¸­ç²å–è©²æ¨¡åž‹çš„å…·é«”è¨­å®šå­—å…¸
    model_config = config['models'][model_key]
    # å¾žè©²å­—å…¸ä¸­ç²å–æ¨¡åž‹é¡žåž‹ï¼Œä»¥æ±ºå®šä½¿ç”¨å“ªå€‹ Wrapper
    model_type = model_config.get('model_type', 'closedsource') # é»˜èªç‚ºé–‰æº

    logging.info(f"æ­£åœ¨å‰µå»ºæ¨¡åž‹ï¼Œéµå: '{model_key}', é¡žåž‹: '{model_type}'")

    if model_type == 'opensource':
        # å‡è¨­æ‰€æœ‰é–‹æºæ¨¡åž‹ Wrapper éƒ½éµå¾ª QwenWrapper çš„æ¨¡å¼
        return QwenWrapper(model_config)
    elif model_type == 'closedsource':
        # å‡è¨­æ‰€æœ‰é–‰æºæ¨¡åž‹ Wrapper éƒ½éµå¾ª GPT4Wrapper çš„æ¨¡å¼
        return GPT4Wrapper(model_config)
    else:
        raise ValueError(f"æœªçŸ¥çš„æ¨¡åž‹é¡žåž‹: {model_type}")

def main():
    """ä¿®æ”¹å¾Œçš„ä¸»å‡½æ•¸ï¼Œæ”¯æŒæ™‚é–“æˆ³è¨˜å¯¦é©—æ—¥èªŒ"""
    
    load_dotenv(override=True)
    
    # --- ç”¨æ–¼åµéŒ¯çš„è‡¨æ™‚ç¨‹å¼ç¢¼ ---
    loaded_key = os.getenv("OPENAI_API_KEY")
    if loaded_key:
        print(f"DEBUG: å·²æˆåŠŸå¾ž .env è¼‰å…¥ API é‡‘é‘°ï¼Œé–‹é ­ç‚º: {loaded_key[:6]}...")
    else:
        print("DEBUG: éŒ¯èª¤ï¼ç„¡æ³•å¾ž .env æª”æ¡ˆä¸­è®€å–åˆ° OPENAI_API_KEYã€‚")
    # --- åµéŒ¯çµæŸ ---
    
    # === 1. å‰µå»ºæ™‚é–“æˆ³è¨˜å¯¦é©—æœƒè©± ===
    timestamp, session_dir = create_timestamped_experiment_session()
    log_file = setup_experiment_logging(session_dir, timestamp)
    
    print(f"ðŸš€ é–‹å§‹æ–°çš„å¯¦é©—æœƒè©±ï¼š{timestamp}")
    print(f"ðŸ“ å¯¦é©—ç›®éŒ„ï¼š{session_dir}")
    print(f"ðŸ“ æ—¥èªŒæ–‡ä»¶ï¼š{log_file}")
    print("-" * 60)
    
    logging.info(f"å¯¦é©—æœƒè©±é–‹å§‹ï¼š{timestamp}")
    logging.info(f"å¯¦é©—ç›®éŒ„ï¼š{session_dir}")
    
    # === 2. è¼‰å…¥é…ç½®å’Œæ•¸æ“š ===
    config = load_config('/home/server/LiangYu/RACE/configs/config.yaml')
    
    # æ›´æ–°è¼¸å‡ºè·¯å¾‘åˆ°æœƒè©±ç›®éŒ„
    results_dir = f"{session_dir}/attack_details"
    summary_path = f"{session_dir}/results/evaluation_summary.json"
    os.makedirs(results_dir, exist_ok=True)
    os.makedirs(f"{session_dir}/results", exist_ok=True)
    
    logging.info("æ­£åœ¨è¼‰å…¥è³‡æ–™é›†...")
    try:
        dataset_path = config['dataset']['path']
        n_examples = config['dataset'].get('n_examples', None)
        
        df = pd.read_csv(dataset_path)
        logging.info(f"æ•¸æ“šé›†è¼‰å…¥æˆåŠŸï¼Œå½¢ç‹€: {df.shape}")
        logging.info(f"æ•¸æ“šé›†åˆ—å: {list(df.columns)}")
        
        # è‡ªå‹•è™•ç†åˆ—åå•é¡Œ
        if 'goal' not in df.columns:
            first_col = df.columns[0]
            df = df.rename(columns={first_col: 'goal'})
            logging.info(f"å·²å°‡åˆ— '{first_col}' é‡å‘½åç‚º 'goal'")
        
        if n_examples and n_examples > 0:
            df = df.head(n_examples)
        
        test_cases = df.to_dict('records')
        
        # è¨˜éŒ„æ•¸æ“šé›†ä¿¡æ¯
        dataset_info = {
            "dataset_path": dataset_path,
            "total_samples": len(test_cases),
            "dataset_columns": list(df.columns),
            "sample_goal": test_cases[0].get('goal', 'N/A')[:100] if test_cases else 'N/A'
        }
        
        logging.info(f"æˆåŠŸè¼‰å…¥ {len(test_cases)} ç­†æ¸¬è©¦æ¡ˆä¾‹")
        
    except Exception as e:
        logging.error(f"è¼‰å…¥è³‡æ–™é›†å¤±æ•—: {e}")
        return
    
    # === 3. ä¿å­˜å¯¦é©—å…ƒæ•¸æ“š ===
    metadata_file = save_experiment_metadata(session_dir, config, dataset_info)
    logging.info(f"å¯¦é©—å…ƒæ•¸æ“šå·²ä¿å­˜ï¼š{metadata_file}")
    
    # === 4. åˆå§‹åŒ–æ¨¡åž‹èˆ‡æ¨¡çµ„ ===
    start_time = time.time()
    
    logging.info("="*20 + " åˆå§‹åŒ–æ¨¡åž‹èˆ‡æ ¸å¿ƒæ¨¡çµ„ " + "="*20)

    # æ ¹æ“š attacker, target, judge çš„ name æ¬„ä½æ‰¾åˆ°å°æ‡‰çš„è¨­å®šéµå
    attacker_key = config['models']['attacker']['name']
    target_key = config['models']['target']['name']
    judge_key = config['models']['judge']['name']
    
    attacker_model = create_model(attacker_key, config)
    target_model = create_model(target_key, config)
    
    logging.info("æ”»æ“Šè€…å’Œç›®æ¨™æ¨¡åž‹å‰µå»ºæˆåŠŸã€‚")

    # åˆå§‹åŒ–æ‰€æœ‰RACEçµ„ä»¶
    reasoning_constructor = ReasoningTaskConstructor(attacker_model)
    ig_calculator = TrueInformationGainCalculator(attacker_model, num_samples=config['self_play'].get('num_samples', 3))
    rejection_detector = RejectionDetector()
    gge_module = PaperAlignedGainGuidedExploration(
        llm_wrapper=attacker_model,
        information_gain_calculator=ig_calculator,
        rejection_detector=rejection_detector,
        num_candidates=config['gain_guided_exploration'].get('num_candidates', 5)
    )
    self_play_optimizer = TrueSelfPlayOptimizer(shadow_model=attacker_model, num_simulations=config['self_play'].get('num_simulations', 3))
    rejection_feedback = RejectionFeedback(attacker_model)
    logging.info("æ‰€æœ‰RACEæ”»æ“Šçµ„ä»¶åˆå§‹åŒ–æˆåŠŸã€‚")
    
    executor = PaperAlignedRACEExecutor(
        base_model=attacker_model,
        reasoning_constructor=reasoning_constructor,
        information_gain_calculator=ig_calculator,
        gge_module=gge_module,
        self_play_optimizer=self_play_optimizer,
        rejection_feedback=rejection_feedback
    )
    logging.info("è«–æ–‡å°é½Šçš„RACEåŸ·è¡Œå™¨åˆå§‹åŒ–æˆåŠŸã€‚")

    all_attack_results = []

    # === 5. æ”»æ“ŠåŸ·è¡ŒéšŽæ®µ ===
    logging.info("="*20 + " é–‹å§‹åŸ·è¡ŒRACEæ”»æ“Šå¾ªç’° " + "="*20)
    for i, test_case in enumerate(tqdm(test_cases, desc="åŸ·è¡Œæ”»æ“Š")):
        logging.info(f"--- æ­£åœ¨è™•ç†æ”»æ“Šæ¡ˆä¾‹ {i+1}/{len(test_cases)} ---")
        goal_text = test_case.get('goal', 'Unknown')
        logging.info(f"ç›®æ¨™è¡Œç‚º: {goal_text}")
        
        attack_result = executor.execute_paper_aligned_attack(
            target_query=goal_text,
            conversation_limit=config['attack_state_machine'].get('max_turns', 5),
            target_model=target_model,
            enable_defense=config['defense'].get('use_sentry', False) or config['defense'].get('use_guardian', False)
        )
        all_attack_results.append(attack_result)

        detail_path = os.path.join(results_dir, f"attack_case_{i+1:03d}.json")
        with open(detail_path, 'w', encoding='utf-8') as f:
            json.dump(attack_result, f, ensure_ascii=False, indent=4)

    logging.info("æ‰€æœ‰æ”»æ“Šæ¡ˆä¾‹è™•ç†å®Œç•¢ã€‚")

    # === 6. æœ€çµ‚è©•ä¼°éšŽæ®µ ===
    logging.info("="*20 + " é–‹å§‹åŸ·è¡Œæœ€çµ‚çš„è«–æ–‡æ¨™æº–è©•ä¼° " + "="*20)
    judge_model = create_model(judge_key, config)
    logging.info("è©•ä¼°æ¨¡åž‹å‰µå»ºæˆåŠŸã€‚")

    adapter = RACESystemEvaluationAdapter(judge_model)
    final_evaluation = adapter.evaluate_race_attack_results(all_attack_results)

    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(final_evaluation, f, ensure_ascii=False, indent=4)
    logging.info(f"è©•ä¼°å®Œæˆï¼æœ€çµ‚è©•ä¼°å ±å‘Šå·²å„²å­˜è‡³: {summary_path}")

    # === 7. ç”Ÿæˆå¯¦é©—ç¸½çµ ===
    execution_time = time.time() - start_time
    
    summary_file = generate_experiment_summary(
        session_dir, all_attack_results, final_evaluation, execution_time
    )
    
    # === 8. å¯¦é©—å®Œæˆå ±å‘Š ===
    logging.info("=" * 60)
    logging.info("ðŸŽ‰ å¯¦é©—å®Œæˆï¼")
    logging.info(f"ðŸ“Š å¯¦é©—ç¸½çµï¼š{summary_file}")
    logging.info(f"â±ï¸  ç¸½åŸ·è¡Œæ™‚é–“ï¼š{execution_time/60:.2f} åˆ†é˜")
    logging.info(f"ðŸ“ æ‰€æœ‰çµæžœä¿å­˜åœ¨ï¼š{session_dir}")
    
    print("\n" + "=" * 60)
    print("ðŸŽ‰ å¯¦é©—å®Œæˆï¼")
    print(f"ðŸ“ å¯¦é©—çµæžœä¿å­˜åœ¨ï¼š{session_dir}")
    print(f"ðŸ“Š æŸ¥çœ‹ç¸½çµï¼š{summary_file}")
    print(f"ðŸ“ æŸ¥çœ‹æ—¥èªŒï¼š{log_file}")
    print("=" * 60)

    # === 9. å¯¦é©—æ‘˜è¦é¡¯ç¤º ===
    logging.info("="*20 + " å¯¦é©—æµç¨‹çµæŸ " + "="*20)
    print("\nå¯¦é©—æ‘˜è¦:")
    summary = final_evaluation.get('evaluation_summary', {})
    print(f"  ç¸½æ”»æ“Šæ¬¡æ•¸: {summary.get('total_attacks')}")
    print(f"  æ”»æ“ŠæˆåŠŸçŽ‡ (ASR): {summary.get('asr_percentage', 0.0):.2f}%")
    print(f"  å¹³å‡å±å®³æŒ‡æ•¸ (HRI): {summary.get('average_hri', 0.0):.2f}")
    print(f"\nè©³ç´°æ”»æ“Šæ—¥èªŒä½æ–¼: {results_dir}")
    print(f"ç¸½é«”è©•ä¼°å ±å‘Šä½æ–¼: {summary_path}")

def list_experiment_history():
    """åˆ—å‡ºæ­·å²å¯¦é©—è¨˜éŒ„"""
    
    experiments_dir = "experiments"
    if not os.path.exists(experiments_dir):
        print("é‚„æ²’æœ‰å¯¦é©—è¨˜éŒ„")
        return
    
    experiments = [d for d in os.listdir(experiments_dir) 
                  if d.startswith('exp_') and os.path.isdir(f"{experiments_dir}/{d}")]
    
    experiments.sort(reverse=True)  # æœ€æ–°çš„åœ¨å‰
    
    print("ðŸ“š å¯¦é©—æ­·å²è¨˜éŒ„ï¼š")
    print("-" * 80)
    
    for exp in experiments[:10]:  # é¡¯ç¤ºæœ€è¿‘10å€‹å¯¦é©—
        exp_dir = f"{experiments_dir}/{exp}"
        summary_file = f"{exp_dir}/EXPERIMENT_SUMMARY.json"
        
        if os.path.exists(summary_file):
            with open(summary_file, 'r', encoding='utf-8') as f:
                summary = json.load(f)
            
            stats = summary.get('attack_statistics', {})
            print(f"ðŸ”¬ {exp}")
            print(f"   â”œâ”€ ç¸½æ”»æ“Šï¼š{stats.get('total_attacks', 'N/A')}")
            print(f"   â”œâ”€ æˆåŠŸçŽ‡ï¼š{stats.get('success_rate', 'N/A')}%")
            print(f"   â”œâ”€ å¹³å‡è¼ªæ•¸ï¼š{stats.get('average_turns_per_attack', 'N/A')}")
            print(f"   â””â”€ ç›®éŒ„ï¼š{exp_dir}")
        else:
            print(f"ðŸ”¬ {exp} (é€²è¡Œä¸­æˆ–ç•°å¸¸çµæŸ)")
        
        print()

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "history":
        list_experiment_history()
    else:
        main()
